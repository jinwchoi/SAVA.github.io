<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106544143-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-106544143-3');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <meta name="author" content="chengao">
  <title>Mitigating Scene Bias</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet">

  <meta property="og:image" content="images/overview.png" />
</head>
<body>

  <div class="navbar-fixed">

    <nav class="grey darken-4" role="navigation">
      <div class="nav-wrapper container"><a id="logo-container" href="#" class="brand-logo"></a>
        <ul class="left hide-on-med-and-down">
          <li><a class="nav-item waves-effect waves-light" href="#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#abstract">Abstract</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#paper">Paper</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#download">Download</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#network">Network</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#results">Results</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#reference">References</a></li>
        </ul>

        <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
      </div>
    </nav>
  </div>





  <div class="section no-pad-bot" id="index-banner">
    <div class="container scrollspy" id="home">

      <h4 class="header center black-text">Why Can't I Dance in the Mall?<br>
      Learning to Mitigate Scene Bias in Action Recognition</h4>

      <br>

      <div class="row center">
        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://sites.google.com/site/jchoivision/" target="blank">Jinwoo Choi</a></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="http://chengao.vision/" target="blank">Chen Gao</a></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://josephcmessou.weebly.com/about.html" target="blank">Joseph Messou</a></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://filebox.ece.vt.edu/~jbhuang/" target="blank">Jia-Bin Huang</a></div>
        </h5>


        <h5 class="header s12">
          <div class="school"><a href="http://www.vt.edu/" target="blank">Virginia Tech</a></div>
        </h5>

        <h6 class="row center">
          <div>In Neural Information Processing Systems (NeurIPS), 2019</div>
        </h6>

      </div>


  </div>


  <div class="container">

    <div class="section">

      <!--   Icon Section   -->
      <div class="row center">
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <!-- <img class="responsive-img" src="images/gao2018hoi.gif"> -->
          <video class="responsive-video" autoplay loop controls>
            <source src="images/teaser.mp4" type="video/mp4">
          </video>
        </div>
<!--         <div class="col s8 offset-s2 note">
          HOI Detection Results on TBBT with our <span class="emphasis">iCAN</span> model
        </div> -->
      </div>

    </div>

    <br>

    <div class="row section scrollspy" id="abstract">
      <div class="title">Abstract</div>
      Human activities often occur in specific scene contexts, e.g. playing basketball on a basketball court. Training a model using existing video datasets thus inevitably captures and leverages such bias (instead of using the actual discriminative cues). The learned representation may not generalize well to new action classes or different tasks. In this paper, we propose to mitigate scene bias for video representation learning. Specifically, we augment the standard cross-entropy loss for action classification with 1) an adversarial loss for scene types and 2) a human mask confusion loss for videos where the human actors are masked out. These two losses encourage learning representations that are unable to predict the scene types and the correct actions when there is no evidence. We validate the effectiveness of our method by transferring our pre-trained model to three different tasks, including action classification, temporal localization, and spatio-temporal action detection. Our results show consistent improvement over the baseline model without debiasing.
    </div>

    <div class="row section scrollspy" id="paper">
      <div class="title">Papers</div>
      <br>

      <div class="row">

        <div class="col l12 m6 s12 center">
          <a href="http://papers.nips.cc/paper/8372-why-cant-i-dance-in-the-mall-learning-to-mitigate-scene-bias-in-action-recognition.pdf" target="_blank">
            <img src="images/icon_pdf.png">
          </a>
          <br>
          <a href="http://papers.nips.cc/paper/8372-why-cant-i-dance-in-the-mall-learning-to-mitigate-scene-bias-in-action-recognition.pdf" target="_blank">NeurIPS 2019</a>
        </div>

      </div>

    </div>


    <div class="row">
      <div class="subtitle">Citation</div>
      <p>Jinwoo Choi, Chen Gao, Joseph Messou, Jia-Bin Huang, "Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition", in Proceedings of Neural Information Processing Systems (NeurIPS), 2019.</p>

      <br>

      <div class="subtitle">Bibtex</div>
      <pre>
@inproceedings{Choi-NeurIPS-2019,
    author    = {Choi, Jinwoo and Gao, Chen and Messou, Joseph CE and Huang, Jia-Bin},
    title     = {Why Can't I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition},
    booktitle = {Advances in Neural Information Processing Systems},
    year      = {2019}
}
      </pre>
    </div>


    <div class="section row scrollspy" id="download">
      <div class="title">Download</div>
      <div class="row">

        <div class="col l12 m6 s12 center">
          <a href="https://github.com/vt-vl-lab/SDN" target="_blank">
            <img src="images/github.png">
          </a>
          <br>
          <a href="https://github.com/vt-vl-lab/SDN" target="_blank">Code</a>
          <!-- <div>Training code and pre-trained models</div> -->
        </div>

    </div>


    <br>
    <div class="section row scrollspy" id="network">
      <div class="title">Overview</div>
      <br>
      <div class="row center">
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <img class="responsive-img" src="images/overview.png">
        </div>
      </div>

    </div>

<br>
<br>
<!-- Results -->
    <div class="section row scrollspy" id="results">
      <div class="title">Results</div>

      <div class="row center">
        <div class="subtitle"><a>Class activation maps (CAM) on the HMDB-51 (first row) and UCF-101 (second row) datasets.</a></div>
        <br>
        <div class="col s12">
          <a><img class="responsive-img" src="images/CAM.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a>Action Classification</a></div>
        <div class="col l6 offset-l3 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/action_classification.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a>Temporal Action Localization</a></div>
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/temporal_action_localization.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a> Spatio-Temporal Action Detection</a></div>
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/spatio-temporal_action_detection.png"></a>
        </div>
      </div>

    </div>


<!-- Ref -->
    <div class="row section scrollspy" id="reference">
      <div class="title">References</div>
      <ul>
        <li>&bull;
          Yang Wang and Minh Hoai, “<a href="" target="blank">Pulling actions out of context: Explicit separation for effective combination</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yingwei Li, Yi Li, and Nuno Vasconcelos, “<a href="" target="blank">Resound: Towards action recognition without representation bias.</a>”, In ECCV, 2018.
        </li>

        <li>&bull;
          Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Val Gool, “<a href="" target="blank">Temporal segment networks: Towards good practices for deep action recognition.</a>”, In ECCV, 2016.
        </li>

        <li>&bull;
          Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh, “<a href="" target="blank">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, and Rahul Sukthankar, “<a href="" target="blank">Rethinking the faster r-cnn architecture for temporal action localization</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin, “<a href="" target="blank">Temporal action detection with structured segment networks</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Huijuan Xu, Abir Das, and Kate Saenko, “<a href="" target="blank">R-C3D: Region convolutional 3d network for temporal activity detection</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang, “<a href="" target="blank">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</a>”, In CVPR, 2017.
        </li>

        <li>&bull;
          Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and Cordelia Schmid, “<a href="" target="blank">Action tubelet detector for spatio-temporal action localization</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy, “<a href="" target="blank">Rethinking spatiotemporal feature learning for video understanding</a>”, In ECCV, 2018.
        </li>

        <li>&bull;
          Gurkirt Singh, Suman Saha, and Fabio Cuzzolin, “<a href="" target="blank">Online real time multiple spatiotemporal action localisa- tion and prediction on a single platform</a>”, In ICCV, 2017.
        </li>

      </ul>
    </div>

  </div>


<!-- Credit -->
  <footer class="page-footer grey lighten-3">


    <div class="footer-copyright center black-text">
      We thank <a href="http://graduatestudents.ucmerced.edu/wlai24/">Jason Lai</a> for providing this wonderful website template.
    </div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>
