<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-106544143-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-106544143-3');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
  <meta name="author" content="chengao">
  <title>SAVA</title>

  <!-- CSS  -->
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link href="css/materialize.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/style.css" type="text/css" rel="stylesheet" media="screen,projection"/>
  <link href="css/font-awesome.min.css" rel="stylesheet">

  <meta property="og:image" content="images/overview.png" />
</head>
<body>

  <div class="navbar-fixed">

    <nav class="grey darken-4" role="navigation">
      <div class="nav-wrapper container"><a id="logo-container" href="#" class="brand-logo"></a>
        <ul class="left hide-on-med-and-down">
          <li><a class="nav-item waves-effect waves-light" href="#home">Home</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#abstract">Abstract</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#paper">Paper</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#download">Download</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#network">Network</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#results">Results</a></li>
          <li><a class="nav-item waves-effect waves-light" href="#reference">References</a></li>
        </ul>

        <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
      </div>
    </nav>
  </div>





  <div class="section no-pad-bot" id="index-banner">
    <div class="container scrollspy" id="home">

      <h4 class="header center black-text">Shuffle and Attend: Video Domain Adaptation</h4>

      <br>

      <div class="row center">
        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://sites.google.com/site/jchoivision/" target="blank">Jinwoo Choi</a><sup>1</sup></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://grvsharma.com/" target="blank">Gaurav Sharma</a><sup>2</sup></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://samschulter.github.io/" target="blank">Samuel Schulter</a><sup>2</sup></div>
        </h5>

        <h5 class="header col l3 m4 s12">
          <div class="author"><a href="https://filebox.ece.vt.edu/~jbhuang/" target="blank">Jia-Bin Huang</a><sup>1</sup></div>
        </h5>


        <div class="row center">
          <span class="affiliation" style="margin-left: 0px;"><a href="http://www.vt.edu/" target="blank"><sup>1</sup>Virginia Tech</a></span>
          
          <span class="affiliation"><a href="http://www.nec-labs.com/" target="blank"><sup>2</sup>NEC Laboratories America</a></span>
        </div>

        <h6 class="row center">
          <div>In European Conference on Computer Vision (ECCV), 2020</div>
        </h6>

      </div>


  </div>


  <div class="container">

    <div class="section">

      <!--   Icon Section   -->
      <div class="row center">
        <div class="col l8 offset-l2 m3 offset-m1 s12">
          <video class="responsive-video" autoplay loop controls>
            <source src="https://filebox.ece.vt.edu/~jinchoi/files/sava/1min_spotlight.mp4" type="video/mp4">
          </video>
          <div class="center-align subtitle">1min introduction</div>
        </div>

        <div class="col l8 offset-l2 m3 offset-m1 s12">
          <video class="responsive-video" autoplay loop controls>
            <source src="https://filebox.ece.vt.edu/~jinchoi/files/sava/10min_video.mp4" type="video/mp4">
          </video>
          <div class="center-align subtitle">Full video</div>
        </div>

      </div>

    </div>

    <br>

    <div class="row section scrollspy" id="abstract">
      <div class="title">Abstract</div>
      We address the problem of domain adaptation in videos for the task of human action recognition. Inspired by image-based domain adaptation, we can perform video adaptation by aligning the features of frames or clips of source and target videos. However, equally aligning all clips is sub-optimal as not all clips are informative for the task. As the first novelty, we propose an attention mechanism which focuses on more discriminative clips and directly optimizes for video-level (\cf clip-level) alignment. As the backgrounds are often very different between source and target, the source background-corrupted model adapts poorly to target domain videos. To alleviate this, as a second novelty, we propose to use the clip order prediction as an auxiliary task. The clip order prediction loss, when combined with domain adversarial loss, encourages learning of representations which focus on the humans and objects involved in the actions, rather than the uninformative and widely differing (between source and target) backgrounds. We empirically show that both components contribute positively towards adaptation performance. We report state-of-the-art performances on two out of three challenging public benchmarks, two based on the UCF and HMDB datasets, and one on Kinetics to NEC-Drone datasets. We also support the intuitions and the results with qualitative results.
    </div>

    <div class="row section scrollspy" id="paper">
      <div class="title">Papers</div>
      <br>

      <div class="row">

        <div class="col l12 m6 s12 center">
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570664.pdf" target="_blank">
            <img src="images/icon_pdf.png">
          </a>
          <br>
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570664.pdf" target="_blank">ECCV 2020</a>
        </div>

      </div>

    </div>


    <div class="row">
      <div class="subtitle">Citation</div>
      <p>Jinwoo Choi, Gaurav Sharma, Samuel Schulter, Jia-Bin Huang, "Shuffle and Attend: Video Domain Adaptation", in Proceedings of European Conference on Computer Vision (ECCV), 2020.</p>

      <br>

      <div class="subtitle">Bibtex</div>
      <pre>
@inproceedings{Choi-ECCV-2020,
    author    = {Choi, Jinwoo and Sharma, Gaurav and Schulter, Samuel and Huang, Jia-Bin},
    title     = {Shuffle and Attend: Video Domain Adaptation},
    booktitle = {ECCV},
    year      = {2020}
}   </pre>
    </div>


    <div class="section row scrollspy" id="download">
      <div class="title">Download</div>
      <div class="row">

        <div class="col l12 m6 s12 center">
          <a href="https://github.com/vt-vl-lab/SDN" target="_blank">
            <img src="images/github.png">
          </a>
          <br>
          <a href="https://github.com/vt-vl-lab/SDN" target="_blank">Code</a>
          <!-- <div>Training code and pre-trained models</div> -->
        </div>

    </div>


    <br>
    <div class="section row scrollspy" id="network">
      <div class="title">Overview</div>
      <br>
      <div class="row center">
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <img class="responsive-img" src="images/overview.png">
        </div>
      </div>

    </div>

<br>
<br>
<!-- Results -->
    <div class="section row scrollspy" id="results">
      <div class="title">Results</div>

      <div class="row center">
        <div class="subtitle"><a>Class activation maps (CAM) on the HMDB-51 (first row) and UCF-101 (second row) datasets.</a></div>
        <br>
        <div class="col s12">
          <a><img class="responsive-img" src="images/CAM.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a>Action Classification</a></div>
        <div class="col l6 offset-l3 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/action_classification.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a>Temporal Action Localization</a></div>
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/temporal_action_localization.png"></a>
        </div>
      </div>

      <div class="row center">
        <div class="subtitle"><a> Spatio-Temporal Action Detection</a></div>
        <div class="col l8 offset-l2 m10 offset-m1 s12">
          <a><img class="responsive-img" src="images/spatio-temporal_action_detection.png"></a>
        </div>
      </div>

    </div>


<!-- Ref -->
    <div class="row section scrollspy" id="reference">
      <div class="title">References</div>
      <ul>
        <li>&bull;
          Yang Wang and Minh Hoai, “<a href="" target="blank">Pulling actions out of context: Explicit separation for effective combination</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yingwei Li, Yi Li, and Nuno Vasconcelos, “<a href="" target="blank">Resound: Towards action recognition without representation bias.</a>”, In ECCV, 2018.
        </li>

        <li>&bull;
          Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Val Gool, “<a href="" target="blank">Temporal segment networks: Towards good practices for deep action recognition.</a>”, In ECCV, 2016.
        </li>

        <li>&bull;
          Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh, “<a href="" target="blank">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A. Ross, Jia Deng, and Rahul Sukthankar, “<a href="" target="blank">Rethinking the faster r-cnn architecture for temporal action localization</a>”, In CVPR, 2018.
        </li>

        <li>&bull;
          Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin, “<a href="" target="blank">Temporal action detection with structured segment networks</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Huijuan Xu, Abir Das, and Kate Saenko, “<a href="" target="blank">R-C3D: Region convolutional 3d network for temporal activity detection</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Zheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang, “<a href="" target="blank">Cdc: Convolutional-de-convolutional networks for precise temporal action localization in untrimmed videos</a>”, In CVPR, 2017.
        </li>

        <li>&bull;
          Vicky Kalogeiton, Philippe Weinzaepfel, Vittorio Ferrari, and Cordelia Schmid, “<a href="" target="blank">Action tubelet detector for spatio-temporal action localization</a>”, In ICCV, 2017.
        </li>

        <li>&bull;
          Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy, “<a href="" target="blank">Rethinking spatiotemporal feature learning for video understanding</a>”, In ECCV, 2018.
        </li>

        <li>&bull;
          Gurkirt Singh, Suman Saha, and Fabio Cuzzolin, “<a href="" target="blank">Online real time multiple spatiotemporal action localisa- tion and prediction on a single platform</a>”, In ICCV, 2017.
        </li>

      </ul>
    </div>

  </div>


<!-- Credit -->
  <footer class="page-footer grey lighten-3">


    <div class="footer-copyright center black-text">
      We thank <a href="http://graduatestudents.ucmerced.edu/wlai24/">Jason Lai</a> for providing this wonderful website template.
    </div>
  </footer>


  <!--  Scripts-->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="js/materialize.js"></script>
  <script src="js/init.js"></script>

  </body>
</html>
